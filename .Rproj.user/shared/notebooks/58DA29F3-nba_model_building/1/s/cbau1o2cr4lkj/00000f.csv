"0","# LINEAR REGRESSION "
"0","lm_model <- linear_reg() %>% "
"0","  set_engine(""lm"")"
"0",""
"0","# K NEAREST NEIGHBORS"
"0","# Tuning the number of neighbors"
"0","knn_model <- nearest_neighbor(neighbors = tune()) %>% "
"0","  set_mode(""regression"") %>% "
"0","  set_engine(""kknn"")"
"0",""
"0","# RIDGE REGRESSION"
"0","ridge_spec <- linear_reg(mixture = 0, "
"0","                         penalty = tune()) %>% "
"0","  set_mode(""regression"") %>% "
"0","  set_engine(""glmnet"")"
"0",""
"0","# LASSO REGRESSION"
"0","lasso_spec <- linear_reg(penalty = tune(), "
"0","                         mixture = 1) %>% "
"0","  set_mode(""regression"") %>% "
"0","  set_engine(""glmnet"")"
"0",""
"0","# POLYNOMIAL REGRESSION"
"0","# Adjusting the recipe because the tuning parameter must be added in the recipe for polynomial regression"
"0","poly_recipe <- nba_recipe %>% "
"0","  step_poly(age, gp, gs, mp, x3p_2, x2p_2, e_fg, ft_2, orb, drb, ast, stl, blk, tov, pf, pts, degree = tune())"
"0",""
"0","poly_spec <- linear_reg() %>% "
"0","  set_mode(""regression"") %>% "
"0","  set_engine(""lm"")"
"0",""
"0","# ELASTIC NET"
"0","# Tuning penalty and mixture"
"0","en_spec <- linear_reg(penalty = tune(), "
"0","                           mixture = tune()) %>% "
"0","  set_mode(""regression"") %>% "
"0","  set_engine(""glmnet"")"
"0",""
"0","# RANDOM FOREST"
"0","# Tuning mtry (number of predictors), trees, and min_n (number of minimum values in each node)"
"0","rf_spec <- rand_forest(mtry = tune(), "
"0","                       trees = tune(), "
"0","                       min_n = tune()) %>% "
"0","  set_engine(""ranger"", importance = ""impurity"") %>% "
"0","  set_mode(""regression"")"
"0",""
"0","# BOOSTED TREES"
"0","# Tuning trees, learn_rate (the learning rate), and min_n"
"0","boosted_spec <- boost_tree(mtry = tune(),"
"0","                           trees = tune(),"
"0","                           learn_rate = tune()) %>%"
"0","  set_engine(""xgboost"") %>%"
"0","  set_mode(""regression"")"
